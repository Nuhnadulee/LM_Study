{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# bert-base-uncased is bert model provided by huggingface\n",
    "unmasker = pipeline('fill-mask',model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': \"hello i'm a korean university student from south korea.\",\n",
       "  'score': 0.19418807327747345,\n",
       "  'token': 4759,\n",
       "  'token_str': 'korean'},\n",
       " {'sequence': \"hello i'm a young university student from south korea.\",\n",
       "  'score': 0.05836235359311104,\n",
       "  'token': 2402,\n",
       "  'token_str': 'young'},\n",
       " {'sequence': \"hello i'm a fellow university student from south korea.\",\n",
       "  'score': 0.03686646744608879,\n",
       "  'token': 3507,\n",
       "  'token_str': 'fellow'},\n",
       " {'sequence': \"hello i'm a chinese university student from south korea.\",\n",
       "  'score': 0.03416597098112106,\n",
       "  'token': 2822,\n",
       "  'token_str': 'chinese'},\n",
       " {'sequence': \"hello i'm a visiting university student from south korea.\",\n",
       "  'score': 0.03217637538909912,\n",
       "  'token': 5873,\n",
       "  'token_str': 'visiting'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Hello I'm a [MASK] university student from South Korea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3, 13), dtype=int32, numpy=\n",
      "array([[  101,  2003,  2023,  1037,  9398,  6251,  1029,   102,     0,\n",
      "            0,     0,     0,     0],\n",
      "       [  101,  1045,  2572,  3076,  2770,  2000, 10930, 12325,  2072,\n",
      "         2118,  2311,  1012,   102],\n",
      "       [  101,  2026,  5440,  4730,  2653,  2003,  1039,  1009,  1009,\n",
      "         1012,   102,     0,     0]])>, 'token_type_ids': <tf.Tensor: shape=(3, 13), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(3, 13), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])>}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model= TFBertModel.from_pretrained('bert-base-uncased')\n",
    "# tokenizer -> dataloader(preprocessing data) -> model\n",
    "text1 =\"Is this a valid sentence?\"\n",
    "text2 = \"I am student running to Yonsei University building.\"\n",
    "text3 = \" My favorite Programming Language is C++.\"\n",
    "text = [text1,text2,text3]\n",
    "encoded_inputs = tokenizer(text, return_tensors='tf',return_attention_mask=True,padding=True,truncation=True)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 13), dtype=int32, numpy=\n",
       "array([[  101,  2003,  2023,  1037,  9398,  6251,  1029,   102,     0,\n",
       "            0,     0,     0,     0],\n",
       "       [  101,  1045,  2572,  3076,  2770,  2000, 10930, 12325,  2072,\n",
       "         2118,  2311,  1012,   102],\n",
       "       [  101,  2026,  5440,  4730,  2653,  2003,  1039,  1009,  1009,\n",
       "         1012,   102,     0,     0]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] is this a valid sentence? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] i am student running to yonsei university building. [SEP]\n",
      "[CLS] my favorite programming language is c + +. [SEP] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(encoded_inputs['input_ids'][0]))\n",
    "print(tokenizer.decode(encoded_inputs['input_ids'][1]))\n",
    "print(tokenizer.decode(encoded_inputs['input_ids'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 101, 2054, 2003, 2115, 2171, 1029,  102, 2026, 2171, 2003, 7367,\n",
      "        5575, 5643, 1012,  102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]])>, 'attention_mask': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}\n"
     ]
    }
   ],
   "source": [
    "encoded_input2 = tokenizer(\"What is your name?\",\"My name is Seungone.\",return_tensors='tf')\n",
    "print(encoded_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] what is your name? [SEP] my name is seungone. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(encoded_input2['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor: shape=(3, 13, 768), dtype=float32, numpy=\n",
      "array([[[ 0.1584683 ,  0.05394202, -0.02173148, ..., -0.1263333 ,\n",
      "          0.06896812,  0.46639672],\n",
      "        [ 0.12491301, -0.1220137 , -0.03282016, ...,  0.3831069 ,\n",
      "          1.0131848 ,  0.6159238 ],\n",
      "        [-0.20566858, -0.6244838 ,  0.16080046, ..., -0.00229507,\n",
      "         -0.05117407,  0.5581342 ],\n",
      "        ...,\n",
      "        [ 0.22871093,  0.44744945,  0.3578546 , ...,  0.46337926,\n",
      "          0.10052502, -0.2444902 ],\n",
      "        [ 0.18982568, -0.05153685, -0.08534762, ...,  0.3760352 ,\n",
      "          0.197602  , -0.27288702],\n",
      "        [ 0.1412916 , -0.01042851, -0.01311123, ...,  0.36208832,\n",
      "          0.18744272, -0.21668495]],\n",
      "\n",
      "       [[-0.22801131,  0.15609686,  0.22225544, ..., -0.577901  ,\n",
      "          0.35622063,  0.18715926],\n",
      "        [-0.16532849, -0.3931642 ,  0.09108079, ..., -0.5639909 ,\n",
      "          0.07929863, -0.29892743],\n",
      "        [-0.3986258 , -0.07625765,  0.6550277 , ..., -0.6394473 ,\n",
      "         -0.02087855, -0.04348911],\n",
      "        ...,\n",
      "        [ 0.7989109 , -0.00488814,  0.1144812 , ..., -0.8205821 ,\n",
      "         -0.1688754 , -0.27963817],\n",
      "        [-0.52072173, -0.5619536 , -0.10707913, ..., -0.20739725,\n",
      "          0.1605132 , -0.80419934],\n",
      "        [ 0.6672145 ,  0.01286071, -0.06178375, ..., -0.01073188,\n",
      "         -0.38118887, -0.16385065]],\n",
      "\n",
      "       [[-0.05409688,  0.06028312, -0.31289005, ..., -0.19278517,\n",
      "          0.17588183,  0.8003695 ],\n",
      "        [ 0.20993368, -0.25410283, -0.2210229 , ..., -0.05778479,\n",
      "          0.03122615,  0.5200929 ],\n",
      "        [ 0.09455551,  0.22572473,  0.1967863 , ..., -0.10307858,\n",
      "          0.07740697,  0.42560345],\n",
      "        ...,\n",
      "        [ 0.428047  ,  0.02009438,  0.1980676 , ...,  0.38901466,\n",
      "         -0.6555018 , -0.18332328],\n",
      "        [ 0.03925106, -0.0317933 ,  0.15678586, ...,  0.29969496,\n",
      "          0.02135993,  0.54023015],\n",
      "        [-0.13839093, -0.3477006 ,  0.06529973, ...,  0.48509985,\n",
      "          0.01171008,  0.4221092 ]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\n",
      "array([[-0.9054412 , -0.24784759, -0.46824443, ..., -0.4176028 ,\n",
      "        -0.6105537 ,  0.9119092 ],\n",
      "       [-0.9127721 , -0.62599826, -0.9887003 , ..., -0.97545004,\n",
      "        -0.7618873 ,  0.93983114],\n",
      "       [-0.8769176 , -0.48585215, -0.9049975 , ..., -0.78686345,\n",
      "        -0.6910988 ,  0.8807093 ]], dtype=float32)>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "output = model(encoded_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFMaskedLMOutput(loss=<tf.Tensor: shape=(9,), dtype=float32, numpy=\n",
      "array([1.2178121e+01, 5.3454580e+00, 9.5798366e-04, 6.0269046e-03,\n",
      "       3.3788753e-03, 5.6930067e-04, 8.1031382e-01, 4.1392818e-04,\n",
      "       2.3580479e+01], dtype=float32)>, logits=<tf.Tensor: shape=(1, 9, 28996), dtype=float32, numpy=\n",
      "array([[[ -7.1545234,  -6.9931316,  -7.182646 , ...,  -5.9123926,\n",
      "          -5.6732936,  -5.9854193],\n",
      "        [ -8.018968 ,  -8.13193  ,  -8.050915 , ...,  -6.5679274,\n",
      "          -6.4058185,  -6.8997593],\n",
      "        [ -4.977202 ,  -6.1780906,  -6.066909 , ...,  -5.636203 ,\n",
      "          -4.6602654,  -5.124104 ],\n",
      "        ...,\n",
      "        [ -3.4420216,  -3.2557287,  -3.5732915, ...,  -2.460596 ,\n",
      "          -2.6494863,  -3.195194 ],\n",
      "        [-10.588984 , -10.462034 , -11.718096 , ...,  -7.4646177,\n",
      "          -9.954243 ,  -8.39268  ],\n",
      "        [-14.889968 , -14.887321 , -14.45686  , ..., -11.658825 ,\n",
      "         -13.0151005, -11.607314 ]]], dtype=float32)>, hidden_states=None, attentions=None)\n",
      "\n",
      "tf.Tensor(\n",
      "[1.2178121e+01 5.3454580e+00 9.5798366e-04 6.0269046e-03 3.3788753e-03\n",
      " 5.6930067e-04 8.1031382e-01 4.1392818e-04 2.3580479e+01], shape=(9,), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[[ -7.1545234  -6.9931316  -7.182646  ...  -5.9123926  -5.6732936\n",
      "    -5.9854193]\n",
      "  [ -8.018968   -8.13193    -8.050915  ...  -6.5679274  -6.4058185\n",
      "    -6.8997593]\n",
      "  [ -4.977202   -6.1780906  -6.066909  ...  -5.636203   -4.6602654\n",
      "    -5.124104 ]\n",
      "  ...\n",
      "  [ -3.4420216  -3.2557287  -3.5732915 ...  -2.460596   -2.6494863\n",
      "    -3.195194 ]\n",
      "  [-10.588984  -10.462034  -11.718096  ...  -7.4646177  -9.954243\n",
      "    -8.39268  ]\n",
      "  [-14.889968  -14.887321  -14.45686   ... -11.658825  -13.0151005\n",
      "   -11.607314 ]]], shape=(1, 9, 28996), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"tf\")\n",
    "inputs[\"labels\"] = tokenizer(\"The capital of France is Paris.\", return_tensors=\"tf\")[\"input_ids\"]\n",
    "\n",
    "outputs = model(inputs)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(outputs)\n",
    "print()\n",
    "print(loss)\n",
    "print()\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': 'The capital of France is Paris.', 'score': 0.44471848011016846, 'token': 2123, 'token_str': 'P a r i s'}, {'sequence': 'The capital of France is Lyon.', 'score': 0.09396772086620331, 'token': 10067, 'token_str': 'L y o n'}, {'sequence': 'The capital of France is Toulouse.', 'score': 0.0823521688580513, 'token': 18367, 'token_str': 'T o u l o u s e'}, {'sequence': 'The capital of France is Lille.', 'score': 0.07515732944011688, 'token': 25411, 'token_str': 'L i l l e'}, {'sequence': 'The capital of France is Marseille.', 'score': 0.05692743510007858, 'token': 17851, 'token_str': 'M a r s e i l l e'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import FillMaskPipeline\n",
    "\n",
    "pipeline = FillMaskPipeline(model,tokenizer)\n",
    "\n",
    "result = pipeline(\"The capital of France is [MASK].\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
